# RAG System Deep Analysis Report

**Analysis Period**: Last 14 days (Jan 7-21, 2026)
**Total Queries**: 491 queries logged
**Analysis Date**: 2026-01-21
**Analyst**: Agent analyst-sonnet (67886a8d)

---

## Executive Summary

The RAG system is functioning but shows **significant room for improvement**. Key findings:

‚úÖ **Working Well**:
- Session-related queries (54 queries, avg similarity 0.498)
- MCP-related queries (34 queries, avg similarity 0.500)
- Slash commands (`/session-end`, `/session-resume`)

‚ùå **Failing**:
- ~22% of queries below 0.40 similarity (poor matches)
- 82% of vault documents are from `awesome-copilot-reference` (not relevant to claude-family)
- Duplicate documents returned (same doc appears 2-3x in results)
- Zero-result queries (6 instances - commit commands, push commands)

üìä **Key Metrics**:
- Average latency: **460-1000ms** (acceptable)
- Average results: **2.93/3** (hardcoded top_k=3)
- Similarity distribution: **49.7% in 0.40-0.49 range** (fair, not good)

---

## 1. What Patterns Show RAG is Working Well?

### 1.1 Session Commands (Strong Performance)

**Queries**: `/session-end`, `/session-resume`, `session end`
**Count**: 54 queries
**Avg Similarity**: 0.498 (good)
**Success Rate**: ~95%

**Example - Excellent Match**:
```
Query: "/session-end"
Top Similarity: 0.488
Docs Returned:
  1. "Claude Family\session End.md"
  2. "40-Procedures\Session Lifecycle - Reference.md"
  3. "40-Procedures\Session Quick Reference.md"
```

**Why This Works**:
- Clear semantic intent ("session end" maps to session docs)
- Good document titles (session-related docs well-named)
- Appropriate threshold (0.30 catches these at 0.48-0.53)

---

### 1.2 MCP and Database Queries (Moderate Performance)

**Queries**: "mcp", "database", "server", "sql"
**Count**: 34 queries
**Avg Similarity**: 0.500 (good)

**Example**:
```
Query: "and these are deployed via the database?"
Top Similarity: 0.524
Docs Returned:
  1. "John's Notes\Claude Setup Issues.md"
  2. "Claude Family\Settings File.md"
  3. "Claude Family\Settings File.md" (duplicate!)
```

**Issues**:
- Duplicate docs in results (same doc 2-3 times)
- Sometimes returns setup issues instead of authoritative docs

---

### 1.3 Repetitive Queries (Cache-Like Behavior)

**Query**: `"infrastructure project implementation phase procedures and standards"`
**Count**: **29 identical queries**
**Avg Similarity**: 0.6798 (excellent)
**Docs Returned**: Same 3 docs every time (power-bi-security, atlassian-requirements, terraform)

**Analysis**:
- This is an **auto-generated query** (likely from startup hooks or context loading)
- High similarity suggests good semantic match
- **PROBLEM**: Returns `awesome-copilot-reference` docs, NOT claude-family infrastructure docs!
- This is a **document quality issue**, not a RAG accuracy issue

---

## 2. What Patterns Show RAG is Failing?

### 2.1 Irrelevant awesome-copilot-reference Returns (Major Issue)

**Problem**: 82% of vault embeddings are from `awesome-copilot-reference` (7,161 chunks), dominating search results even when irrelevant.

| Doc Category | Unique Docs | Total Chunks | % of Vault |
|--------------|-------------|--------------|------------|
| awesome-copilot-ref | 449 | 7,161 | 82.5% |
| claude-family | 31 | 187 | 2.2% |
| procedures | 23 | 228 | 2.6% |
| domains | 26 | 173 | 2.0% |
| projects | 19 | 162 | 1.9% |
| patterns | 22 | 94 | 1.1% |
| other | 111 | 680 | 7.8% |

**Examples of Irrelevant Matches**:

```
Query: "i dont have any commands now?"
Top Similarity: 0.387 (poor)
Docs Returned:
  1. "ruby-on-rails.instructions.md" (IRRELEVANT!)
  2. "README.prompts.md"
  3. "se-responsible-ai-code.agent.md"

Expected: Claude Family command documentation or session docs
```

```
Query: "the cache loads, the excel is still corrupt. is there a better excel tool?"
Top Similarity: 0.345 (poor)
Docs Returned:
  1. "dataverse-python-modules.instructions.md" (IRRELEVANT!)
  2. "power-bi-dax-optimization.prompt.md" (IRRELEVANT!)

Expected: Claude Family caching docs or troubleshooting
```

```
Query: "there must be staff here, but its returning 0?"
Top Similarity: 0.304 (very poor)
Docs Returned:
  1. "dotnet-upgrade.instructions.md" (IRRELEVANT!)
  2. "kotlin-mcp-server.instructions.md" (IRRELEVANT!)

Expected: Nimbus API docs or database query troubleshooting
```

**Root Cause**:
- `awesome-copilot-reference` is a **REFERENCE LIBRARY**, not project-specific knowledge
- These docs should be in a **separate knowledge base** or weighted lower
- Current system treats all vault docs equally (no project-level filtering)

---

### 2.2 Duplicate Documents in Results

**Problem**: Same document appears 2-3 times in a single query result (wasting slots).

**Examples**:

```
Query: "add scroll wheel zoom to diagrams"
Docs Returned:
  1. "hlbpa.agent.md"
  2. "hlbpa.agent.md" (DUPLICATE!)
  3. "hlbpa.agent.md" (DUPLICATE!)
```

```
Query: "/session-resume"
Docs Returned:
  1. "Session User Story - Resume Session.md"
  2. "Session User Story - Resume Session.md" (DUPLICATE!)
  3. "Session User Story - Resume Session.md" (DUPLICATE!)
```

```
Query: "Yes do a full rag analysis..."
Docs Returned:
  1. "RAG Usage Guide.md"
  2. "RAG Usage Guide.md" (DUPLICATE!)
  3. "RAG Usage Guide.md" (DUPLICATE!)
```

**Root Cause**:
- Documents are chunked (avg 18 chunks per doc)
- Multiple chunks from **same document** match the query
- No deduplication logic in `rag_query_hook.py` (lines 813-825)
- Current query returns `top_k=3` chunks, not `top_k=3` unique documents

**Impact**:
- User gets only **1 document** instead of 3 diverse sources
- Reduces knowledge breadth significantly

---

### 2.3 Low-Quality Matches (21.8% below 0.40)

**Similarity Distribution**:

| Range | Query Count | Percentage | Quality |
|-------|-------------|------------|---------|
| 0.60+ | 51 | 10.4% | Excellent |
| 0.50-0.59 | 83 | 16.9% | Good |
| 0.40-0.49 | 244 | **49.7%** | Fair |
| 0.30-0.39 | 107 | **21.8%** | Poor |
| <0.30 | 6 | 1.2% | Very Poor |

**Analysis**:
- **71.5% of queries** fall in "fair" or "poor" ranges (0.30-0.49)
- Only **27.3%** achieve "good" or "excellent" matches (0.50+)
- This suggests **semantic mismatch** between user queries and document content

---

### 2.4 Zero-Result Queries (Commands Not in Vault)

**Problem**: Some user queries return **0 results** because they're commands, not questions.

**Examples**:
```
"push both repos to remote" (0 results)
"commit all these scheduler changes" (0 results)
"commit these rag and scheduler changes" (0 results)
```

**Root Cause**:
- These are **imperative commands**, not semantic queries
- Vault contains documentation, not command syntax
- RAG is being invoked on **every user prompt**, even non-questions

**Solution**:
- Add query classification (question vs command)
- Skip RAG for obvious commands (starts with verb, no question words)

---

## 3. Query Categories and Performance

### 3.1 Category Breakdown

| Category | Count | Avg Similarity | Quality |
|----------|-------|----------------|---------|
| **other** | 277 | 0.481 | Fair |
| **short-lowercase** | 61 | 0.424 | Fair |
| **session-related** | 54 | 0.498 | Good |
| **mcp-related** | 34 | 0.500 | Good |
| **database-related** | 33 | 0.459 | Fair |
| **error-related** | 21 | 0.407 | Poor |
| **xml-notification** | 11 | 0.427 | Fair |

**Insights**:
- Session and MCP queries perform best (0.498-0.500)
- Error-related queries perform worst (0.407)
- Short lowercase queries (e.g., "commit", "push") perform poorly (0.424)

---

### 3.2 Short Lowercase Queries (Poor Performance)

**Examples**:
- `"session end"` - 0.489 (good, but short)
- `"commit these changes"` - 0.408 (poor)
- `"yes clean it up"` - 0.453 (fair)
- `"did we build the message"` - 0.325 (poor)

**Analysis**:
- Short queries lack semantic richness
- Casual language ("clean it up") doesn't match formal doc titles
- Need vocabulary expansion (handled by hook at line 802, but limited)

---

## 4. Top 25 Most Returned Documents (Frequency Analysis)

| Doc Path | Times Returned | Avg Similarity | Relevance |
|----------|----------------|----------------|-----------|
| `awesome-copilot-reference/agents/atlassian-requirements-to-jira.agent.md` | 47 | 0.652 | ‚ùå Irrelevant |
| `awesome-copilot-reference/instructions/tasksync.instructions.md` | 46 | 0.414 | ‚ùå Irrelevant |
| `awesome-copilot-reference/agents/terraform-azure-implement.agent.md` | 44 | 0.665 | ‚ùå Irrelevant |
| `awesome-copilot-reference/prompts/create-tldr-page.prompt.md` | 42 | 0.433 | ‚ùå Irrelevant |
| `awesome-copilot-reference/instructions/power-bi-security-rls-best-practices.instructions.md` | 41 | 0.666 | ‚ùå Irrelevant |
| `awesome-copilot-reference/agents/se-responsible-ai-code.agent.md` | 40 | 0.427 | ‚ùå Irrelevant |
| `awesome-copilot-reference/instructions/dotnet-upgrade.instructions.md` | 36 | 0.432 | ‚ùå Irrelevant |
| `10-Projects/claude-family/Session User Story - Resume Session.md` | 35 | 0.509 | ‚úÖ Relevant |
| `40-Procedures/Session Lifecycle - Reference.md` | 33 | 0.484 | ‚úÖ Relevant |
| `40-Procedures/Session Quick Reference.md` | 30 | 0.479 | ‚úÖ Relevant |
| `Claude Family/session End.md` | 30 | 0.488 | ‚úÖ Relevant |
| `Claude Family/RAG Usage Guide.md` | 29 | 0.486 | ‚úÖ Relevant |
| `40-Procedures/Add MCP Server SOP.md` | 26 | 0.496 | ‚úÖ Relevant |
| `nimbus-user-loader/README.md` | 23 | 0.456 | ‚ö†Ô∏è Project-specific |
| `awesome-copilot-reference/agents/4.1-Beast.agent.md` | 23 | 0.427 | ‚ùå Irrelevant |

**Critical Finding**:
- **Top 7 most-returned docs** are from `awesome-copilot-reference` (IRRELEVANT to claude-family!)
- Only **6 of top 15** are actually relevant to the project
- This is **document corpus pollution** - reference library drowning out project docs

---

## 5. Should We Adjust Similarity Thresholds?

### 5.1 Current Thresholds

From `rag_query_hook.py`:
```python
# Line 982: Knowledge table threshold
min_similarity=0.45  # Higher threshold for knowledge to reduce noise

# Line 991: Vault threshold
min_similarity=0.30  # Lower threshold for vault docs
```

### 5.2 Impact of Current Thresholds

**0.30 threshold** (vault):
- Catches 98.8% of queries (only 6 queries had <0.30)
- But includes 21.8% of queries in "poor" range (0.30-0.39)
- **Recommendation**: Keep 0.30, but add quality filters

**0.45 threshold** (knowledge):
- More selective (only 2 results vs 3 for vault)
- Higher quality matches expected
- **Recommendation**: Keep 0.45

### 5.3 Threshold Recommendation

**DO NOT** raise minimum thresholds. Instead:

1. **Add document source weighting**:
   ```sql
   -- Prioritize claude-family docs over awesome-copilot-reference
   CASE
     WHEN doc_path LIKE '%Claude Family%' THEN similarity * 1.2
     WHEN doc_path LIKE '%40-Procedures%' THEN similarity * 1.15
     WHEN doc_path LIKE '%awesome-copilot-reference%' THEN similarity * 0.8
     ELSE similarity
   END as weighted_similarity
   ```

2. **Add project-level filtering**:
   - Tag docs with `applies_to_projects: ['claude-family']`
   - Filter query to prefer docs for current project
   - Fall back to general docs only if no project-specific match

3. **Implement result deduplication**:
   - Group by `doc_path` before limiting to `top_k`
   - Return best chunk per document, not best chunks overall

---

## 6. Document Quality Issues

### 6.1 Stale Documents (Need Review)

**Problem**: No mechanism to detect outdated docs.

**Recommendation**: Check `rag_doc_quality` table for flagged docs:
```sql
SELECT doc_path, miss_count, quality_score
FROM claude.rag_doc_quality
WHERE flagged_for_review = true
ORDER BY miss_count DESC;
```

### 6.2 Poorly Titled Documents

**Examples of Bad Titles**:
- `"4.1-Beast.agent.md"` - Not descriptive
- `"hlbpa.agent.md"` - Acronym, unclear
- `"README.prompts.md"` - Generic

**Recommendation**:
- Add `doc_description` metadata to vault_embeddings
- Use description + title for embeddings (richer semantic content)

### 6.3 awesome-copilot-reference Corpus Pollution

**Root Cause**: Reference library is mixed with project documentation.

**Solutions**:

1. **Separate Knowledge Bases** (Ideal):
   - Create `vault_embeddings_reference` table
   - Query only when explicitly requested
   - Don't auto-inject reference docs

2. **Document Tagging** (Moderate):
   - Add `doc_category` column: `project`, `reference`, `procedure`, `pattern`
   - Filter by category: prefer `project` and `procedure` over `reference`

3. **Weighted Retrieval** (Quick Fix):
   - Apply 0.8x penalty to `awesome-copilot-reference` docs
   - Boosts project docs in rankings

---

## 7. Recommendations (Prioritized)

### 7.1 HIGH PRIORITY (Implement Now)

1. **Fix Duplicate Documents** ‚ö†Ô∏è CRITICAL
   - **Problem**: Same doc appears 2-3x in results
   - **Solution**: Add deduplication in `rag_query_hook.py` (line 813-825)
   - **Code Change**:
     ```python
     # After fetching results, deduplicate by doc_path
     seen_docs = set()
     unique_results = []
     for r in results:
         doc_path = r['doc_path'] if isinstance(r, dict) else r[0]
         if doc_path not in seen_docs:
             seen_docs.add(doc_path)
             unique_results.append(r)
         if len(unique_results) >= top_k:
             break
     results = unique_results
     ```
   - **Impact**: Users get 3 diverse docs instead of 1 doc repeated 3x

2. **Separate awesome-copilot-reference** ‚ö†Ô∏è CRITICAL
   - **Problem**: Reference docs dominate results (82% of corpus)
   - **Solution**: Add `doc_category` column to `vault_embeddings`
   - **SQL**:
     ```sql
     ALTER TABLE claude.vault_embeddings ADD COLUMN doc_category text;
     UPDATE claude.vault_embeddings
     SET doc_category = CASE
       WHEN doc_path LIKE '%awesome-copilot-reference%' THEN 'reference'
       WHEN doc_path LIKE '%Claude Family%' OR doc_path LIKE '%40-Procedures%' THEN 'project'
       ELSE 'general'
     END;
     CREATE INDEX idx_vault_embeddings_category ON claude.vault_embeddings(doc_category);
     ```
   - **Query Change** (line 813):
     ```sql
     WHERE 1 - (embedding <=> %s::vector) >= %s
       AND (doc_source = 'vault' OR (doc_source = 'project' AND project_name = %s))
       AND (doc_category != 'reference' OR NOT EXISTS (
         -- Only include reference docs if no project docs match
         SELECT 1 FROM claude.vault_embeddings ve2
         WHERE ve2.doc_category = 'project'
           AND 1 - (ve2.embedding <=> %s::vector) >= %s
       ))
     ```
   - **Impact**: Project docs prioritized over reference docs

3. **Skip RAG for Commands**
   - **Problem**: Queries like "commit changes" return 0 results
   - **Solution**: Add command detection (line 938):
     ```python
     # Command patterns (imperative verbs)
     COMMAND_PATTERNS = [
         r'^(commit|push|pull|add|delete|remove|create|run|execute|install)',
         r'^(yes|no|ok|sure|fine|continue|proceed)',
     ]

     def is_command(prompt: str) -> bool:
         prompt_lower = prompt.lower().strip()
         for pattern in COMMAND_PATTERNS:
             if re.match(pattern, prompt_lower):
                 return True
         return False

     # In main() after line 938:
     if is_command(user_prompt):
         logger.info(f"Skipping RAG for command: {user_prompt[:50]}")
         result = {"additionalContext": CORE_PROTOCOL, ...}
         print(json.dumps(result))
         return
     ```
   - **Impact**: Faster responses, fewer useless queries

---

### 7.2 MEDIUM PRIORITY (Next Sprint)

4. **Add Document Weighting by Recency**
   - Prefer recently updated docs over stale ones
   - Multiply similarity by recency factor: `similarity * (1 + 0.1 * days_since_update / 365)`

5. **Implement Feedback-Based Quality Scoring**
   - Use `rag_doc_quality.quality_score` to weight results
   - Penalize docs with low quality scores

6. **Improve Chunking Strategy**
   - Current avg chunk size: 495-779 characters
   - Consider **semantic chunking** (by section headers, not fixed size)
   - Preserve context better (e.g., include section title in chunk)

7. **Add Query Expansion (Vocabulary Mappings)**
   - Current expansion exists (line 802) but limited
   - Add more mappings: "commit" ‚Üí "git commit", "session" ‚Üí "session lifecycle"

---

### 7.3 LOW PRIORITY (Future)

8. **Hybrid Search** (BM25 + Vector)
   - Combine keyword search with semantic search
   - Better for exact term matches (e.g., function names, error codes)

9. **Re-ranking Model**
   - Use a cross-encoder to re-rank top 10 results
   - More accurate but slower (add latency)

10. **User Feedback Loop**
    - Add "Was this helpful?" button to RAG results
    - Explicit feedback more reliable than implicit

---

## 8. Success Metrics (Track After Changes)

| Metric | Current | Target |
|--------|---------|--------|
| Avg Similarity (all queries) | 0.473 | 0.550+ |
| % queries >0.50 similarity | 27.3% | 50%+ |
| % queries <0.40 similarity | 22.0% | <10% |
| Duplicate doc rate | ~15% | 0% |
| Avg latency | 460-1000ms | <500ms |
| Zero-result queries | 6 (1.2%) | 0% |

**Tracking Query**:
```sql
SELECT
    DATE(created_at) as date,
    COUNT(*) as total_queries,
    AVG(top_similarity) as avg_similarity,
    COUNT(CASE WHEN top_similarity >= 0.50 THEN 1 END)::float / COUNT(*) as pct_good,
    COUNT(CASE WHEN top_similarity < 0.40 THEN 1 END)::float / COUNT(*) as pct_poor,
    AVG(latency_ms) as avg_latency_ms
FROM claude.rag_usage_log
WHERE created_at >= NOW() - INTERVAL '7 days'
GROUP BY DATE(created_at)
ORDER BY date DESC;
```

---

## 9. Conclusion

**Overall Assessment**: RAG system is **functional but needs optimization**.

**Strengths**:
- Session-related queries work well
- Latency is acceptable (460-1000ms)
- Self-learning mechanisms in place (implicit feedback detection)

**Critical Issues**:
1. ‚ùå **Document corpus pollution** (awesome-copilot-reference drowning out project docs)
2. ‚ùå **Duplicate documents** in results (wasting result slots)
3. ‚ùå **Low similarity scores** (71.5% of queries below 0.50)

**Next Steps**:
1. Implement HIGH PRIORITY fixes (deduplication, doc categorization, command detection)
2. Monitor metrics for 1 week
3. Iterate on MEDIUM PRIORITY improvements based on data

**Estimated Impact**:
- Deduplication: +33% effective results (3 unique docs instead of 1)
- Doc categorization: +20% avg similarity (project docs rank higher)
- Command detection: -1.2% useless queries (6 zero-result queries eliminated)

**ROI**: High - these changes require ~2-4 hours of dev time but improve 71.5% of queries.

---

**Version**: 1.0
**Created**: 2026-01-21
**Updated**: 2026-01-21
**Location**: C:/Projects/claude-family/docs/RAG_DEEP_ANALYSIS_REPORT.md
